{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9RK7eTxIIxX"
      },
      "source": [
        "import pandas as pd                                                         # importando a biblioteca pandas (utilizada para manipulação e análise de dados)\n",
        "import numpy as np                                                          # importando a biblioteca numpy (usada para operações matemáticas e manipulação de arrays)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmo Random Forest\n",
        "\n",
        "## 1- Construção de Árvores de Decisão:\n",
        "\n",
        "A Random Forest constrói múltiplas árvores de decisão durante o tempo de treinamento. Uma Árvore de Decisão é um modelo preditivo que divide repetidamente os dados em subconjuntos baseados em características (features) específicas, visando reduzir a impureza dos dados em cada divisão."
      ],
      "metadata": {
        "id": "atNqQPAMkP1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Agregação por Bootstrap (Bagging):\n",
        "\n",
        "A técnica de agregação por bootstrap, ou simplesmente \"bagging\", é utilizada para treinar cada árvore. Isso envolve:\n",
        "\n",
        "- Amostragem com reposição: Para cada árvore, uma amostra aleatória do conjunto de treinamento original é extraída com reposição. Isso significa que alguns dados podem ser selecionados várias vezes, enquanto outros podem não ser selecionados.\n",
        "- Treinamento das Árvores: Cada árvore é treinada de forma independente usando essas amostras aleatórias."
      ],
      "metadata": {
        "id": "5aHyzxbxwxW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- Diversidade entre Árvores:\n",
        "\n",
        "Para aumentar a diversidade entre as árvores e evitar que todas sigam os mesmos padrões de divisão, a Random Forest introduz aleatoriedade adicional:\n",
        "\n",
        "- Subconjuntos de Características: Ao invés de considerar todas as características disponíveis para cada divisão, apenas um subconjunto aleatório de características é considerado. Esse subconjunto é selecionado aleatoriamente para cada divisão em cada árvore."
      ],
      "metadata": {
        "id": "4WFwfOxdxHyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4- Combinação de Previsões:\n",
        "\n",
        "Após o treinamento, as previsões de todas as árvores são combinadas para gerar a previsão final:\n",
        "- Classificação: A classe final é determinada pela moda das previsões de todas as árvores (a classe mais votada).\n",
        "- Regressão: A previsão final é a média das previsões de todas as árvores."
      ],
      "metadata": {
        "id": "Y3vLO-5Bxeqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed, randrange  # seed: define a semente para o gerador de números aleatórios, garantindo que ele produza a mesma sequência de números toda vez que o código for executado;\n",
        "                                    # randrange: gera um número inteiro aleatório dentro de um intervalo especificado;\n",
        "from csv import reader              # reader: função usada para ler dados de um arquivo CSV;\n",
        "from math import sqrt               # sqrt: função usada para calcular a raiz quadrada de um número."
      ],
      "metadata": {
        "id": "_zlB1MlKxr4F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando um arquivo CSV:\n",
        "\n",
        "def load_csv(filename):                          # Define uma função chamada load_csv para carregar o arquivo CSV a ser lido.\n",
        "    dataset = list()                             # Inicializa uma lista vazia chamada dataset, que será usada para armazenar as linhas do CSV.\n",
        "    with open(filename, 'r') as file:            # Abre o arquivo CSV para leitura ('r'). *O with statement garante que o arquivo será fechado automaticamente após a leitura.\n",
        "        csv_reader = reader(file)                # Cria um objeto 'csv_reader' usando a função 'reader' da biblioteca 'csv'. Este objeto permitirá iterar (percorrer ou passar por cada item de uma sequência de dados) sobre as linhas do arquivo CSV.\n",
        "        for row in csv_reader:                   # Inicia um loop 'for' que itera sobre cada linha (row) no csv_reader.\n",
        "            if not row: continue                 # Verifica se a linha (row) está vazia. Se estiver vazia, usa 'continue' para pular para a próxima iteração do loop, evitando adicionar linhas vazias ao dataset.\n",
        "            dataset.append(row)                  # Adiciona a linha (row) não vazia à lista dataset.\n",
        "    return dataset                               # Após ler todas as linhas do CSV e adicioná-las à lista dataset, a função retorna o dataset."
      ],
      "metadata": {
        "id": "KBNP-n6F_tmC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando um arquivo CSV:\n",
        "\n",
        "def load_csv(filename):                          # Define uma função chamada load_csv para carregar o arquivo CSV a ser lido.\n",
        "    dataset = list()                             # Inicializa uma lista vazia chamada dataset, que será usada para armazenar as linhas do CSV.\n",
        "    with open(filename, 'r') as file:            # Abre o arquivo CSV para leitura ('r'). *O with statement garante que o arquivo será fechado automaticamente após a leitura.\n",
        "        csv_reader = reader(file)                # Cria um objeto 'csv_reader' usando a função 'reader' da biblioteca 'csv'. Este objeto permitirá iterar (percorrer ou passar por cada item de uma sequência de dados) sobre as linhas do arquivo CSV.\n",
        "        for row in csv_reader:                   # Inicia um loop 'for' que itera sobre cada linha (row) no csv_reader.\n",
        "            if not row: continue                 # Verifica se a linha (row) está vazia. Se estiver vazia, usa 'continue' para pular para a próxima iteração do loop, evitando adicionar linhas vazias ao dataset.\n",
        "            dataset.append(row)                  # Adiciona a linha (row) não vazia à lista dataset.\n",
        "    return dataset                               # Após ler todas as linhas do CSV e adicioná-las à lista dataset, a função retorna o dataset."
      ],
      "metadata": {
        "id": "IjuQTXhOyQ-K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os valores de uma coluna de strings para floats:\n",
        "\n",
        "def str_column_to_flt(dataset, column):                          # Define a função 'chamada str_column_to_flt', que converte os valores de uma coluna específica em um dataset de strings para floats.\n",
        "    for row in dataset:                                          # Inicia um loop 'for' que itera sobre cada linha (row) no dataset.\n",
        "        row[column] = float( row[column].strip() )               # Para cada linha, acessa o valor na coluna especificada, remove quaisquer espaços em branco usando strip(), converte o valor para float, e atualiza o valor na linha."
      ],
      "metadata": {
        "id": "aQ07JwnmyTvg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os valores de uma coluna de strings para inteiros:\n",
        "\n",
        "def str_column_to_int(dataset, column):\n",
        "    class_values = [ row[column] for row in dataset ]\n",
        "    unique = set( class_values )\n",
        "    lookup = dict()\n",
        "    for i, value in enumerate( unique ): lookup[value] = i\n",
        "    for row in dataset: row[column] = lookup[row[column]]\n",
        "    return lookup"
      ],
      "metadata": {
        "id": "mmS4UEWRyXV0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo um dataset em k folds para validação cruzada (usada para avaliar a capacidade de generalização de um modelo):\n",
        "\n",
        "from random import randrange\n",
        "\n",
        "def cross_validation_split(dataset, n_folds):            # Define uma função chamada cross_validation_split (dataset que será dividido em folds, número de folds desejado para a divisão do dataset).\n",
        "                                                           # Inicialização das variáveis:\n",
        "    dataset_split = list()                               # dataset_split: uma lista que armazenará os folds criados.\n",
        "    dataset_copy = list(dataset)                         # dataset_copy: uma cópia do dataset original, feito para garantir que não haja alterações no dataset original durante a divisão.\n",
        "    fold_size = len(dataset) / n_folds                   # fold_size: tamanho de cada fold (é calculado dividindo o número total de exemplos pelo número de folds desejado).\n",
        "\n",
        "    for i in range(n_folds):                             # Inicia um loop que será executado n vezes, criando um fold em cada iteração.\n",
        "        fold = list()                                    # Inicializa uma lista vazia para armazenar os exemplos do fold atual.\n",
        "        while len(fold) < fold_size:                     # Enquanto o tamanho do fold atual for menor que o tamanho esperado (fold_size), um loop é executado.\n",
        "            index = randrange(len(dataset_copy))         # Um índice aleatório é escolhido dentro do intervalo válido para o dataset atual (usando randrange(len(dataset_copy))).\n",
        "            fold.append(dataset_copy.pop(index))         # O exemplo correspondente ao índice escolhido é removido de dataset_copy (usando pop(index)) e adicionado ao fold atual.\n",
        "\n",
        "        dataset_split.append(fold)                       # Após preencher o fold atual com exemplos suficientes, ele é adicionado à lista de folds (dataset_split).\n",
        "    return dataset_split                                 # Após criar todos os folds, a função retorna a lista de folds.\n"
      ],
      "metadata": {
        "id": "mVfT5Qf9ya3p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando a precisão do modelo de classificação:\n",
        "\n",
        "def accuracy_metric(actual, predicted):                 # Define uma função chamada accuracy_metric (lista de valores reais, lista de valores previstos pelo modelo).\n",
        "    correct = 0                                         # Inicializa um contador para o número de previsões corretas.\n",
        "    for i in range(len(actual)):                        # Inicia um loop que itera sobre cada par de valores real e previsto.\n",
        "        if actual[i] == predicted[i]: correct += 1      # Verifica se a previsão (predicted[i]) coincide com o valor real (actual[i]), e, se sim, incrementa o contador 'correct'.\n",
        "    return 100. * correct / float( len(actual) )        # Retorna a precisão calculada como a porcentagem de previsões corretas em relação ao total de previsões.\n",
        "                                                        # (multiplica por 100 para obter o valor em percentual)"
      ],
      "metadata": {
        "id": "ypX33Cmpyfvp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliando o algoritmo de aprendizado de máquina usando divisão de validação cruzada:\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):      # Define uma função chamada evaluate_algorithm (dataset, algoritmo de aprendizado de máquina a ser avaliado, número de folds para a validação cruzada, argumentos adicionais que podem ser passados para o algoritmo).\n",
        "    folds = cross_validation_split(dataset, n_folds)             # Usa a função cross_validation_split para dividir o dataset em k (número de subconjuntos) folds.\n",
        "    scores = list()                                              # Inicializa uma lista vazia para armazenar as pontuações de precisão de cada fold.\n",
        "    for fold in folds:                                           # Inicia um loop que itera sobre cada fold criado pela função cross_validation_split.\n",
        "\n",
        "        train_set = list(folds)                                  # Cria o conjunto de treinamento combinando todos os folds, exceto o fold atual.\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "\n",
        "        test_set = list()                                        # Cria o conjunto de teste para o fold atual, removendo o valor de destino para evitar vazamento de informações durante o teste.\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "\n",
        "        predicted = algorithm(train_set, test_set, *args)        # Executa o algoritmo de aprendizado de máquina no conjunto de treinamento e teste, passando quaisquer argumentos adicionais especificados.\n",
        "\n",
        "        actual = [row[-1] for row in fold]                       # Obtém os valores reais do conjunto de teste e calcula a precisão das previsões feitas pelo algoritmo.\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "\n",
        "        scores.append(accuracy)                                  # Adiciona a precisão à lista de pontuações.\n",
        "    return scores                                                # Retorna a lista de pontuações de precisão para todos os folds."
      ],
      "metadata": {
        "id": "vuTkYAY2yjvJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo um dataset em dois grupos com base no valor de um atributo específico:\n",
        "\n",
        "def test_split(index, value, dataset):                          # Define uma função chamada test_split (índice do atributo usado para a divisão, valor do atributo que determina a divisão, dataset a ser dividido).\n",
        "    left, right = list(), list()                                # Inicializa duas listas vazias para os grupos resultantes da divisão: um grupo à esquerda e outro à direita.\n",
        "    for row in dataset:                                         # Inicia um loop que itera sobre cada linha do dataset.\n",
        "        if row[index] < value: left.append(row)                 # Verifica se o valor do atributo na linha é menor que o valor especificado (value), se for, a linha é adicionada ao grupo à esquerda (left).\n",
        "        else: right.append(row)                                 # Caso contrário, a linha é adicionada ao grupo à direita (right).\n",
        "    return left, right                                          # Retorna os grupos resultantes à esquerda e à direita"
      ],
      "metadata": {
        "id": "c1UFD3_UymhO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando o índice Gini (métrica de impureza usada em algoritmos de árvore de decisão para avaliar a qualidade de uma divisão em um dataset) para uma divisão de dataset:\n",
        "\n",
        "def gini_index(groups, class_values):                                                    # Define uma função chamada gini_index (lista de grupos após a divisão do dataset, lista dos valores de classe possíveis).\n",
        "    gini = 0.0                                                                           # Inicializa o índice Gini como 0.\n",
        "    for class_value in class_values:                                                     # Inicia um loop que itera sobre cada valor de classe possível.\n",
        "        for group in groups:                                                             # Inicia um loop que itera sobre cada grupo na divisão do dataset.\n",
        "            size = len(group)                                                            # Calcula o tamanho do grupo atual.\n",
        "            if size == 0: continue                                                       # Verifica se o grupo está vazio. Se estiver, passa para o próximo grupo.\n",
        "            proportion = [row[-1] for row in group].count(class_value) / float(size)     # Calcula a proporção de instâncias no grupo atual que têm o valor de classe atual.\n",
        "            gini += (proportion * (1.0 - proportion))                                    # Adiciona a contribuição do grupo para o índice Gini. Quanto maior a impureza do grupo, maior será a contribuição para o índice Gini.\n",
        "    return gini                                                                          # Retorna o índice Gini final."
      ],
      "metadata": {
        "id": "CVDx7UMDyp7U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionar o melhor ponto de divisão para o dataset: (objetivo é encontrar a melhor maneira de dividir o dataset com base em determinadas características (atributos) para maximizar a pureza dos grupos resultantes).\n",
        "\n",
        "def get_split(dataset, n_features):                                                      # Define uma função chamada get_split (dataset para o qual o ponto de divisão será selecionado, número de atributos a serem considerados durante a seleção do ponto de divisão).\n",
        "    class_values = list(set(row[-1] for row in dataset))                                 # Obtém os valores de classe únicos no dataset.\n",
        "                                                                                            # Inicialização das variáveis para armazenar o melhor ponto de divisão encontrado até agora:\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None                            # b_index: Índice do atributo usado para a divisão. / b_value: Valor do atributo usado para a divisão. / b_score: Índice Gini do melhor ponto de divisão encontrado até agora. / b_groups: Grupos resultantes da melhor divisão encontrada até agora.\n",
        "\n",
        "    features = list()                                                                    # Seleciona aleatoriamente 'n' índices de atributos para consideração durante a seleção do ponto de divisão.\n",
        "    while len(features) < n_features:\n",
        "        index = randrange(len(dataset[0])-1)\n",
        "        if index not in features: features.append(index)\n",
        "\n",
        "    for index in features:                                                               # Inicia um loop que itera sobre cada índice de atributo selecionado.\n",
        "        for row in dataset:                                                              # Inicia um loop que itera sobre cada linha no dataset.\n",
        "            groups = test_split(index, row[index], dataset)                              # Divide o dataset com base no atributo atual e no valor do atributo na linha atual.\n",
        "            gini = gini_index(groups, class_values)                                      # Calcula o índice Gini para a divisão.\n",
        "            if gini < b_score:                                                           # Se o índice Gini calculado for menor que o melhor índice Gini encontrado até agora,\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups    # atualiza as variáveis para o melhor ponto de divisão encontrado até agora.\n",
        "\n",
        "    return {'index': b_index, 'value': b_value, 'groups': b_groups}                      # Retorna um dicionário contendo o índice do atributo usado para a divisão, o valor do atributo usado para a divisão e os grupos resultantes da divisão."
      ],
      "metadata": {
        "id": "WsedQRB9ys7Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando um nó terminal em uma árvore de decisão:  --> quando a construção da árvore de decisão atinge um ponto onde não há mais divisões a serem feitas (por exemplo, todas as instâncias pertencem à mesma classe), um nó terminal é criado e atribuído a ele o valor mais comum da classe dentro desse grupo.\n",
        "\n",
        "def to_terminal(group):                               # Define uma função chamada to_terminal (grupo de instâncias para o qual o nó terminal será criado).\n",
        "    outcomes = [row[-1] for row in group]             # Obtém todas as classes das instâncias no grupo. O índice '-1' é usado para acessar a última coluna, que contém as classes.\n",
        "    return max(set(outcomes), key=outcomes.count)     # Retorna a classe mais comum no grupo, que será atribuída ao nó terminal da árvore de decisão."
      ],
      "metadata": {
        "id": "bOrKFYnKyw7d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando divisões nos nós de uma árvore de decisão: --> responsável por dividir um nó da árvore em dois nós filhos, criando a estrutura de decisão recursivamente.\n",
        "\n",
        "def split(node, max_depth, min_size, n_features, depth): # Define uma função chamada split (nó atual que está sendo dividido, profundidade máxima permitida para a árvore, número mínimo de instâncias permitido em um nó antes de não ser mais dividido, número de características (atributos) a serem consideradas ao fazer uma divisão, profundidade atual na construção da árvore).\n",
        "\n",
        "    left, right = node['groups']                                                # Obtém os grupos resultantes da divisão do nó atual,\n",
        "    del(node['groups'])                                                         # e remove a chave 'groups' do dicionário node.\n",
        "\n",
        "    # check for a no split (Verifica se não há dados à esquerda ou à direita para dividir. Nesse caso, o nó atual é transformado em um nó terminal com o valor da classe mais comum entre os dados combinados à esquerda e à direita).\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "\n",
        "    # check for max depth (Verifica se a profundidade máxima da árvore foi atingida. Se sim, os nós à esquerda e à direita são transformados em nós terminais).\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "\n",
        "    # process left child (1- Verifica se o número de instâncias no nó à esquerda é menor ou igual ao tamanho mínimo especificado).\n",
        "    if len(left) <= min_size:                                             # Se sim, o nó à esquerda é transformado em um nó terminal.\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:                                                                 # Caso contrário, a função get_split é chamada para encontrar a melhor divisão para o nó à esquerda, e o processo é repetido recursivamente.\n",
        "        node['left'] = get_split(left, n_features)\n",
        "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
        "\n",
        "    # process right child (2- Verifica se o número de instâncias no nó à direita é menor ou igual ao tamanho mínimo especificado).\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right, n_features)\n",
        "        split(node['right'], max_depth, min_size, n_features, depth+1)"
      ],
      "metadata": {
        "id": "mHbvr1bxyx_M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construindo uma árvore de decisão usando o algoritmo CART (Classification and Regression Trees):\n",
        "\n",
        "def build_tree(train, max_depth, min_size, n_features):      # Define uma função chamada build_tree (conjunto de treinamento usado para construir a árvore de decisão, profundidade máxima permitida para a árvore, número mínimo de instâncias permitido em um nó antes de não ser mais dividido, número de características (atributos) a serem consideradas ao fazer uma divisão).\n",
        "    root = get_split(train, n_features)                      # Usa a função get_split para encontrar a melhor divisão para o conjunto de treinamento, resultando no nó raiz da árvore de decisão.\n",
        "    split(root, max_depth, min_size, n_features, 1)          # Usa a função split para dividir recursivamente o nó raiz e seus filhos até atingir os critérios de parada especificados pelos parâmetros max_depth e min_size.\n",
        "    return root                                              # Retorna o nó raiz da árvore de decisão construída."
      ],
      "metadata": {
        "id": "6uxdbFUGy1Uc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo previsões usando uma árvore de decisão construída:\n",
        "\n",
        "def predict(node, row):                                                                 # Define uma função chamada predict (nó atual da árvore de decisão, instância de dados (vetor) para a qual uma previsão será feita).\n",
        "\n",
        "    if row[node['index']] < node['value']:                                              # Verifica se o valor do atributo na instância row é menor que o valor de divisão armazenado no nó. Se a condição for verdadeira, a travessia continua para o nó filho à esquerda.\n",
        "        if isinstance(node['left'], dict): return predict(node['left'], row)            # 1- Se o nó filho for um dicionário, isso significa que ainda não é um nó terminal, então a função predict é chamada recursivamente para continuar a travessia pela árvore.\n",
        "        else: return node['left']                                                       # 2- Se o nó filho for um valor terminal (ou seja, não é um dicionário), isso significa que é um nó terminal, e o valor do nó é retornado como a previsão.\n",
        "    else:                                                                               # Caso contrário, a travessia continua para o nó filho à direita.\n",
        "        if isinstance(node['right'], dict): return predict(node['right'], row)          # 1- ~~\n",
        "        else: return node['right']                                                      # 2- ~~"
      ],
      "metadata": {
        "id": "O7uTKJKTy7Pr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma amostra aleatória (subamostra) do conjunto de dados:\n",
        "\n",
        "def subsample(dataset, ratio):                   # Define uma função chamada subsample (conjunto de dados original do qual a subamostra será criada, proporção de instâncias que serão incluídas na subamostra, representada como um valor entre 0 e 1).\n",
        "    sample = list()                              # Cria uma lista vazia chamada sample, usada para armazenar as instâncias selecionadas aleatoriamente do conjunto de dados original.\n",
        "    n_sample = round(len(dataset) * ratio)       # Calcula o tamanho da subamostra multiplicando o número total de instâncias no conjunto de dados pelo valor da proporção ratio. O resultado é arredondado para o inteiro mais próximo.\n",
        "\n",
        "    while len(sample) < n_sample:                # Inicia um loop enquanto o tamanho da subamostra for menor que o tamanho desejado.\n",
        "        index = randrange(len(dataset))          # Seleciona aleatoriamente um índice do conjunto de dados original usando a função randrange.\n",
        "        sample.append(dataset[index])            # Adiciona a instância correspondente ao índice selecionado à subamostra.\n",
        "    return sample                                # Retorna a subamostra criada."
      ],
      "metadata": {
        "id": "As-Oahbly9j6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo previsões utilizando uma lista de árvores de decisão que foram treinadas em diferentes subamostras do conjunto de dados original (método conhecido como \"bagging\" (Bootstrap Aggregating)):\n",
        "\n",
        "def bagging_predict(trees, row):                            # Define uma função chamada bagging_predict (lista de árvores de decisão previamente treinadas, instância de dados (vetor) para a qual uma previsão será feita).\n",
        "\n",
        "    predictions = [predict(tree, row) for tree in trees]    # Usa uma compreensão de lista para iterar sobre cada árvore na lista trees e faz uma previsão para a instância row usando a função predict.\n",
        "                                                            # Armazena todas as previsões em uma lista chamada predictions.\n",
        "\n",
        "    return max(set(predictions), key=predictions.count)     # Usa a função set para obter os valores únicos das previsões.\n",
        "                                                            # Usa a função max com o argumento key=predictions.count para encontrar a previsão mais comum (a moda) na lista de previsões.\n",
        "                                                            # Retorna a previsão mais comum como a previsão final para a instância row."
      ],
      "metadata": {
        "id": "lEY6GJBdy_ww"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "# Defina seu conjunto de treinamento aqui\n",
        "train = [\n",
        "    [1, 'a'],\n",
        "    [2, 'b'],\n",
        "    [3, 'c'],\n",
        "    [4, 'd'],\n",
        "    [5, 'e']\n",
        "]\n",
        "\n",
        "print(\"Tamanho do conjunto de treinamento:\", len(train))\n",
        "print(\"Exemplo do conjunto de treinamento:\", train[:5])\n",
        "\n",
        "\n",
        "# Defina a proporção de amostragem (sample_size)\n",
        "sample_size = 0.8  # Por exemplo, definindo a proporção de amostragem como 80%\n",
        "\n",
        "print(\"Proporção de amostragem (sample_size):\", sample_size)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLlr8Mv21WgF",
        "outputId": "9d70d8ef-05d2-4034-eedf-8a36244a131b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do conjunto de treinamento: 5\n",
            "Exemplo do conjunto de treinamento: [[1, 'a'], [2, 'b'], [3, 'c'], [4, 'd'], [5, 'e']]\n",
            "Proporção de amostragem (sample_size): 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subsample(dataset, ratio):\n",
        "    sample = list()\n",
        "    n_sample = round(len(dataset) * ratio)\n",
        "    print(\"Tamanho da amostra desejada (n_sample):\", n_sample)  # Exibir o tamanho da amostra desejada\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        print(\"Erro: conjunto de dados vazio.\")\n",
        "        return sample\n",
        "\n",
        "    if n_sample <= 0:\n",
        "        return sample\n",
        "\n",
        "    while len(sample) < n_sample:\n",
        "        index = randint(0, len(dataset) - 1)\n",
        "        sample.append(dataset[index])\n",
        "    return sample"
      ],
      "metadata": {
        "id": "g3YjOP0M36vH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Algoritmo Random Forest: --> O algoritmo cria várias árvores de decisão em diferentes subamostras do conjunto de dados original e combina suas previsões para obter uma previsão final robusta e precisa.\n",
        "\n",
        "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
        "    trees = list()                                                                       # Cria uma lista vazia para armazenar as árvores de decisão que serão geradas.\n",
        "    for i in range(n_trees):                                                             # Para cada árvore na floresta (total de n_trees árvores):\n",
        "        sample = subsample(train, sample_size)                                           # Cria uma subamostra do conjunto de treinamento usando a função subsample com a proporção sample_size.\n",
        "        tree = build_tree(sample, max_depth, min_size, n_features)                       # Constrói uma árvore de decisão usando a função build_tree com a subamostra criada e os parâmetros max_depth, min_size e n_features.\n",
        "        trees.append(tree)                                                               # Adiciona a árvore construída à lista trees.\n",
        "    predictions = [bagging_predict(trees, row) for row in test]                          # Faz previsões para cada instância no conjunto de teste usando a função bagging_predict, que combina as previsões de todas as árvores na floresta, e armazena as previsões em uma lista predictions.\n",
        "    return(predictions)                                                                  # Retorna a lista de previsões para o conjunto de teste."
      ],
      "metadata": {
        "id": "El5fiTgRzCSs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação Detalhada\n",
        "- Subamostragem: Cada árvore é treinada em uma subamostra do conjunto de dados de treinamento, criada com reposição (ou seja, a mesma instância pode aparecer várias vezes na mesma subamostra).\n",
        "\n",
        "- Seleção Aleatória de Características: Ao procurar a melhor divisão em cada nó de uma árvore, apenas um subconjunto aleatório de características é considerado (n_features). Isso promove a diversidade entre as árvores e ajuda a reduzir a correlação entre elas.\n",
        "\n",
        "- Combinação das Previsões: As previsões de todas as árvores são combinadas usando votação majoritária (para classificação) ou média (para regressão) para produzir a previsão final.\n",
        "\n",
        "Essa abordagem melhora a precisão e a robustez do modelo, tornando-o menos suscetível a overfitting em comparação com uma única árvore de decisão."
      ],
      "metadata": {
        "id": "1wJBH1mMnyp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDITAR OS CÓDIGOS SÓ DAQUI PRA BAIXOXXOXOXOXOXOXOXOXXOOOO!!!!!!!!!!!!!"
      ],
      "metadata": {
        "id": "GJS2YZSOBPPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados = pd.read_csv(\"/content/car_accidentOFICIAL.csv\")"
      ],
      "metadata": {
        "id": "8pG-tDMN27Sq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dados.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9LBd5ZuPwk2",
        "outputId": "2cc83ca6-a3c0-4205-a777-7500a6e0877d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Count', 'Accident_ID', 'Accident_Date', 'Day_of_Week',\n",
            "       'Junction_Detail', 'Accident_Injuries', 'Light_Conditions',\n",
            "       'Number_of_Casualties', 'Number_of_Vehicles', 'Road_Surface_Conditions',\n",
            "       'Road_Type', 'Speed_limit', 'Hour_of_Accident', 'Urban_or_Rural_Area',\n",
            "       'Weather_Conditions', 'Vehicle_Type'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminando colunas:\n",
        "\n",
        "dados = dados.drop(['Count','Accident_ID', 'Accident_Date', 'Day_of_Week','Light_Conditions',\n",
        "       'Number_of_Casualties', 'Number_of_Vehicles','Speed_limit', 'Weather_Conditions', 'Vehicle_Type'], axis=1)"
      ],
      "metadata": {
        "id": "EsgUtq_5G8AI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dados.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzsbXWDIm7jU",
        "outputId": "ad268390-f6d7-4a7c-a534-6d4501251167"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Junction_Detail', 'Accident_Injuries', 'Road_Surface_Conditions',\n",
            "       'Road_Type', 'Hour_of_Accident', 'Urban_or_Rural_Area'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dados.head())  # Exibindo as primeiras linhas do DataFrame para checar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syUG_V1XHIDi",
        "outputId": "40eaab70-9084-4191-e6e9-f5d3375cb887"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Junction_Detail Accident_Injuries Road_Surface_Conditions  \\\n",
            "0  T or staggered junction           Serious                     Dry   \n",
            "1               Crossroads           Serious             Wet or damp   \n",
            "2  T or staggered junction            Slight                     Dry   \n",
            "3  T or staggered junction           Serious            Frost or ice   \n",
            "4               Crossroads           Serious                     Dry   \n",
            "\n",
            "            Road_Type Hour_of_Accident Urban_or_Rural_Area  \n",
            "0      One way street            12-15               Urban  \n",
            "1  Single carriageway             9-12               Urban  \n",
            "2  Single carriageway            12-15               Urban  \n",
            "3  Single carriageway              6-9               Urban  \n",
            "4  Single carriageway            15-18               Urban  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "USURpgAkqWFU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dados)"
      ],
      "metadata": {
        "id": "Lriw1_on5aD3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('car_accidentOFICIAL.csv', index=False)"
      ],
      "metadata": {
        "id": "naj5R1QA5CL4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('car_accidentOFICIAL.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "L2RA8bv50x6Y",
        "outputId": "e29c1921-5fdb-49c2-b09a-b71e59671989"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_77466a04-c41b-4e5b-a537-ecb8bd720777\", \"car_accidentOFICIAL.csv\", 21262401)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTANDO O ALGORITMO Random Forest:\n",
        "\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "dataset = load_csv('/content/car_accidentOFICIAL (1).csv')\n",
        "\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 10\n",
        "min_size = 1\n",
        "sample_size = 0.8\n",
        "n_features = int(sqrt(len(dataset[0])-1))\n",
        "for n_trees in [1, 5, 10, 50, 100]:\n",
        "    scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
        "    print('Trees: %d' % n_trees)\n",
        "    print('Scores: %s' % scores)\n",
        "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "g9KV5QseAp2d",
        "outputId": "9975e3a1-6919-42af-8511-fe895d55f855"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "empty range for randrange()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c0b4aae53eeb>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_trees\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trees: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_trees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scores: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d5cc9160270a>\u001b[0m in \u001b[0;36mevaluate_algorithm\u001b[0;34m(dataset, algorithm, n_folds, *args)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0;31m# Define uma função chamada evaluate_algorithm (dataset, algoritmo de aprendizado de máquina a ser avaliado, número de folds para a validação cruzada, argumentos adicionais que podem ser passados para o algoritmo).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# Usa a função cross_validation_split para dividir o dataset em k (número de subconjuntos) folds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                              \u001b[0;31m# Inicializa uma lista vazia para armazenar as pontuações de precisão de cada fold.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m:\u001b[0m                                           \u001b[0;31m# Inicia um loop que itera sobre cada fold criado pela função cross_validation_split.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-7f9ee3a73eb3>\u001b[0m in \u001b[0;36mcross_validation_split\u001b[0;34m(dataset, n_folds)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                    \u001b[0;31m# Inicializa uma lista vazia para armazenar os exemplos do fold atual.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfold_size\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0;31m# Enquanto o tamanho do fold atual for menor que o tamanho esperado (fold_size), um loop é executado.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Um índice aleatório é escolhido dentro do intervalo válido para o dataset atual (usando randrange(len(dataset_copy))).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_copy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# O exemplo correspondente ao índice escolhido é removido de dataset_copy (usando pop(index)) e adicionado ao fold atual.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36mrandrange\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mistart\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mistart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"empty range for randrange()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# stop argument supplied.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: empty range for randrange()"
          ]
        }
      ]
    }
  ]
}