{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9RK7eTxIIxX"
      },
      "source": [
        "import pandas as pd                                                         # importando a biblioteca pandas (utilizada para manipulação e análise de dados)\n",
        "import numpy as np                                                          # importando a biblioteca numpy (usada para operações matemáticas e manipulação de arrays)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmo Random Forest\n",
        "\n",
        "## 1- Construção de Árvores de Decisão:\n",
        "\n",
        "A Random Forest constrói múltiplas árvores de decisão durante o tempo de treinamento. Uma Árvore de Decisão é um modelo preditivo que divide repetidamente os dados em subconjuntos baseados em características (features) específicas, visando reduzir a impureza dos dados em cada divisão."
      ],
      "metadata": {
        "id": "atNqQPAMkP1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Agregação por Bootstrap (Bagging):\n",
        "\n",
        "A técnica de agregação por bootstrap, ou simplesmente \"bagging\", é utilizada para treinar cada árvore. Isso envolve:\n",
        "\n",
        "- Amostragem com reposição: Para cada árvore, uma amostra aleatória do conjunto de treinamento original é extraída com reposição. Isso significa que alguns dados podem ser selecionados várias vezes, enquanto outros podem não ser selecionados.\n",
        "- Treinamento das Árvores: Cada árvore é treinada de forma independente usando essas amostras aleatórias."
      ],
      "metadata": {
        "id": "5aHyzxbxwxW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- Diversidade entre Árvores:\n",
        "\n",
        "Para aumentar a diversidade entre as árvores e evitar que todas sigam os mesmos padrões de divisão, a Random Forest introduz aleatoriedade adicional:\n",
        "\n",
        "- Subconjuntos de Características: Ao invés de considerar todas as características disponíveis para cada divisão, apenas um subconjunto aleatório de características é considerado. Esse subconjunto é selecionado aleatoriamente para cada divisão em cada árvore."
      ],
      "metadata": {
        "id": "4WFwfOxdxHyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4- Combinação de Previsões:\n",
        "\n",
        "Após o treinamento, as previsões de todas as árvores são combinadas para gerar a previsão final:\n",
        "- Classificação: A classe final é determinada pela moda das previsões de todas as árvores (a classe mais votada).\n",
        "- Regressão: A previsão final é a média das previsões de todas as árvores."
      ],
      "metadata": {
        "id": "Y3vLO-5Bxeqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed, randrange  # seed: define a semente para o gerador de números aleatórios, garantindo que ele produza a mesma sequência de números toda vez que o código for executado;\n",
        "                                    # randrange: gera um número inteiro aleatório dentro de um intervalo especificado;\n",
        "from csv import reader              # reader: função usada para ler dados de um arquivo CSV;\n",
        "from math import sqrt               # sqrt: função usada para calcular a raiz quadrada de um número."
      ],
      "metadata": {
        "id": "_zlB1MlKxr4F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando um arquivo CSV: (**hmmm na próxima célula tem outra forma, não consegui fazer essa aqui dar certo)\n",
        "\n",
        "def load_csv(filename):                          # Define uma função chamada load_csv para carregar o arquivo CSV a ser lido.\n",
        "    dataset = list()                             # Inicializa uma lista vazia chamada dataset, que será usada para armazenar as linhas do CSV.\n",
        "    with open(filename, 'r') as file:            # Abre o arquivo CSV para leitura ('r'). *O with statement garante que o arquivo será fechado automaticamente após a leitura.\n",
        "        csv_reader = reader(file)                # Cria um objeto 'csv_reader' usando a função 'reader' da biblioteca 'csv'. Este objeto permitirá iterar (percorrer ou passar por cada item de uma sequência de dados) sobre as linhas do arquivo CSV.\n",
        "        for row in csv_reader:                   # Inicia um loop 'for' que itera sobre cada linha (row) no csv_reader.\n",
        "            if not row: continue                 # Verifica se a linha (row) está vazia. Se estiver vazia, usa 'continue' para pular para a próxima iteração do loop, evitando adicionar linhas vazias ao dataset.\n",
        "            dataset.append(row)                  # Adiciona a linha (row) não vazia à lista dataset.\n",
        "    return dataset                               # Após ler todas as linhas do CSV e adicioná-las à lista dataset, a função retorna o dataset."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "IjuQTXhOyQ-K",
        "outputId": "67155321-8fb9-465b-dce8-5e96282aaafa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-13-0c367dd05927>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-0c367dd05927>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def load_csv(\"/content/car_accidentOFICIAL.csv\")                           # Define uma função chamada load_csv para carregar o arquivo CSV a ser lido.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/car_accidentOFICIAL.csv\")"
      ],
      "metadata": {
        "id": "8pG-tDMN27Sq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os valores de uma coluna de strings para floats: (**acho que a gente não vai usar isso, serve pra preparar dados para análise estatística ou modelagem, em que é necessário trabalhar com valores numéricos.)\n",
        "\n",
        "def str_column_to_flt(dataset, column):                          # Define a função 'chamada str_column_to_flt', que converte os valores de uma coluna específica em um dataset de strings para floats.\n",
        "    for row in dataset:                                          # Inicia um loop 'for' que itera sobre cada linha (row) no dataset.\n",
        "        row[column] = float( row[column].strip() )               # Para cada linha, acessa o valor na coluna especificada, remove quaisquer espaços em branco usando strip(), converte o valor para float, e atualiza o valor na linha."
      ],
      "metadata": {
        "id": "aQ07JwnmyTvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os valores de uma coluna de strings para inteiros: (**também acho que não usaremos):\n",
        "\n",
        "def str_column_to_int(dataset, column):\n",
        "    class_values = [ row[column] for row in dataset ]\n",
        "    unique = set( class_values )\n",
        "    lookup = dict()\n",
        "    for i, value in enumerate( unique ): lookup[value] = i\n",
        "    for row in dataset: row[column] = lookup[row[column]]\n",
        "    return lookup"
      ],
      "metadata": {
        "id": "mmS4UEWRyXV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo um dataset em k folds para validação cruzada (usada para avaliar a capacidade de generalização de um modelo):\n",
        "\n",
        "def cross_validation_split(dataset, n_folds):            # Define uma função chamada cross_validation_split (dataset que será dividido em folds, número de folds desejado para a divisão do dataset).\n",
        "                                                           # Inicialização das variáveis:\n",
        "    dataset_split = list()                               # dataset_split: uma lista que armazenará os folds criados.\n",
        "    dataset_copy = list(dataset)                         # dataset_copy: uma cópia do dataset original, feito para garantir que não haja alterações no dataset original durante a divisão.\n",
        "    fold_size = len(dataset) / n_folds                   # fold_size: tamanho de cada fold (é calculado dividindo o número total de exemplos pelo número de folds desejado).\n",
        "\n",
        "    for i in xrange(n_folds):                            # Inicia um loop que será executado n vezes, criando um fold em cada iteração.\n",
        "        fold = list()                                    # Inicializa uma lista vazia para armazenar os exemplos do fold atual.\n",
        "        while len(fold) < fold_size:                     # Enquanto o tamanho do fold atual for menor que o tamanho esperado (fold_size), um loop é executado.\n",
        "            index = randrange(len(dataset_copy))         # Um índice aleatório é escolhido dentro do intervalo válido para o dataset atual (usando randrange(len(dataset_copy))).\n",
        "            fold.append(dataset_copy.pop(index))         # O exemplo correspondente ao índice escolhido é removido de dataset_copy (usando pop(index)) e adicionado ao fold atual.\n",
        "\n",
        "        dataset_split.append(fold)                       # Após preencher o fold atual com exemplos suficientes, ele é adicionado à lista de folds (dataset_split).\n",
        "    return dataset_split                                 # Após criar todos os folds, a função retorna a lista de folds."
      ],
      "metadata": {
        "id": "mVfT5Qf9ya3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando a precisão do modelo de classificação:\n",
        "\n",
        "def accuracy_metric(actual, predicted):                 # Define uma função chamada accuracy_metric (lista de valores reais, lista de valores previstos pelo modelo).\n",
        "    correct = 0                                         # Inicializa um contador para o número de previsões corretas.\n",
        "    for i in xrange(len(actual)):                       # Inicia um loop que itera sobre cada par de valores real e previsto.\n",
        "        if actual[i] == predicted[i]: correct += 1      # Verifica se a previsão (predicted[i]) coincide com o valor real (actual[i]), e, se sim, incrementa o contador 'correct'.\n",
        "    return 100. * correct / float( len(actual) )        # Retorna a precisão calculada como a porcentagem de previsões corretas em relação ao total de previsões.\n",
        "                                                        # (multiplica por 100 para obter o valor em percentual)"
      ],
      "metadata": {
        "id": "ypX33Cmpyfvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliando o algoritmo de aprendizado de máquina usando divisão de validação cruzada:\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):      # Define uma função chamada evaluate_algorithm (dataset, algoritmo de aprendizado de máquina a ser avaliado, número de folds para a validação cruzada, argumentos adicionais que podem ser passados para o algoritmo).\n",
        "    folds = cross_validation_split(dataset, n_folds)             # Usa a função cross_validation_split para dividir o dataset em k (número de subconjuntos) folds.\n",
        "    scores = list()                                              # Inicializa uma lista vazia para armazenar as pontuações de precisão de cada fold.\n",
        "    for fold in folds:                                           # Inicia um loop que itera sobre cada fold criado pela função cross_validation_split.\n",
        "\n",
        "        train_set = list(folds)                                  # Cria o conjunto de treinamento combinando todos os folds, exceto o fold atual.\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "\n",
        "        test_set = list()                                        # Cria o conjunto de teste para o fold atual, removendo o valor de destino para evitar vazamento de informações durante o teste.\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "\n",
        "        predicted = algorithm(train_set, test_set, *args)        # Executa o algoritmo de aprendizado de máquina no conjunto de treinamento e teste, passando quaisquer argumentos adicionais especificados.\n",
        "\n",
        "        actual = [row[-1] for row in fold]                       # Obtém os valores reais do conjunto de teste e calcula a precisão das previsões feitas pelo algoritmo.\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "\n",
        "        scores.append(accuracy)                                  # Adiciona a precisão à lista de pontuações.\n",
        "    return scores                                                # Retorna a lista de pontuações de precisão para todos os folds."
      ],
      "metadata": {
        "id": "vuTkYAY2yjvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo um dataset em dois grupos com base no valor de um atributo específico:\n",
        "\n",
        "def test_split(index, value, dataset):                          # Define uma função chamada test_split (índice do atributo usado para a divisão, valor do atributo que determina a divisão, dataset a ser dividido).\n",
        "    left, right = list(), list()                                # Inicializa duas listas vazias para os grupos resultantes da divisão: um grupo à esquerda e outro à direita.\n",
        "    for row in dataset:                                         # Inicia um loop que itera sobre cada linha do dataset.\n",
        "        if row[index] < value: left.append(row)                 # Verifica se o valor do atributo na linha é menor que o valor especificado (value), se for, a linha é adicionada ao grupo à esquerda (left).\n",
        "        else: right.append(row)                                 # Caso contrário, a linha é adicionada ao grupo à direita (right).\n",
        "    return left, right                                          # Retorna os grupos resultantes à esquerda e à direita"
      ],
      "metadata": {
        "id": "c1UFD3_UymhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculan o índice Gini (métrica de impureza usada em algoritmos de árvore de decisão para avaliar a qualidade de uma divisão em um dataset) para uma divisão de dataset:\n",
        "\n",
        "def gini_index(groups, class_values):                                                    # Define uma função chamada gini_index (lista de grupos após a divisão do dataset, lista dos valores de classe possíveis).\n",
        "    gini = 0.0                                                                           # Inicializa o índice Gini como 0.\n",
        "    for class_value in class_values:                                                     # Inicia um loop que itera sobre cada valor de classe possível.\n",
        "        for group in groups:                                                             # Inicia um loop que itera sobre cada grupo na divisão do dataset.\n",
        "            size = len(group)                                                            # Calcula o tamanho do grupo atual.\n",
        "            if size == 0: continue                                                       # Verifica se o grupo está vazio. Se estiver, passa para o próximo grupo.\n",
        "            proportion = [row[-1] for row in group].count(class_value) / float(size)     # Calcula a proporção de instâncias no grupo atual que têm o valor de classe atual.\n",
        "            gini += (proportion * (1.0 - proportion))                                    # Adiciona a contribuição do grupo para o índice Gini. Quanto maior a impureza do grupo, maior será a contribuição para o índice Gini.\n",
        "    return gini                                                                          # Retorna o índice Gini final."
      ],
      "metadata": {
        "id": "CVDx7UMDyp7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionar o melhor ponto de divisão para o dataset: (objetivo é encontrar a melhor maneira de dividir o dataset com base em determinadas características (atributos) para maximizar a pureza dos grupos resultantes).\n",
        "\n",
        "def get_split(dataset, n_features):                                                      # Define uma função chamada get_split (dataset para o qual o ponto de divisão será selecionado, número de atributos a serem considerados durante a seleção do ponto de divisão).\n",
        "    class_values = list(set(row[-1] for row in dataset))                                 # Obtém os valores de classe únicos no dataset.\n",
        "                                                                                            # Inicialização das variáveis para armazenar o melhor ponto de divisão encontrado até agora:\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None                            # b_index: Índice do atributo usado para a divisão. / b_value: Valor do atributo usado para a divisão. / b_score: Índice Gini do melhor ponto de divisão encontrado até agora. / b_groups: Grupos resultantes da melhor divisão encontrada até agora.\n",
        "\n",
        "    features = list()                                                                    # Seleciona aleatoriamente 'n' índices de atributos para consideração durante a seleção do ponto de divisão.\n",
        "    while len(features) < n_features:\n",
        "        index = randrange(len(dataset[0])-1)\n",
        "        if index not in features: features.append(index)\n",
        "\n",
        "    for index in features:                                                               # Inicia um loop que itera sobre cada índice de atributo selecionado.\n",
        "        for row in dataset:                                                              # Inicia um loop que itera sobre cada linha no dataset.\n",
        "            groups = test_split(index, row[index], dataset)                              # Divide o dataset com base no atributo atual e no valor do atributo na linha atual.\n",
        "            gini = gini_index(groups, class_values)                                      # Calcula o índice Gini para a divisão.\n",
        "            if gini < b_score:                                                           # Se o índice Gini calculado for menor que o melhor índice Gini encontrado até agora,\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups    # atualiza as variáveis para o melhor ponto de divisão encontrado até agora.\n",
        "\n",
        "    return {'index': b_index, 'value': b_value, 'groups': b_groups}                      # Retorna um dicionário contendo o índice do atributo usado para a divisão, o valor do atributo usado para a divisão e os grupos resultantes da divisão."
      ],
      "metadata": {
        "id": "WsedQRB9ys7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando um nó terminal em uma árvore de decisão:  --> quando a construção da árvore de decisão atinge um ponto onde não há mais divisões a serem feitas (por exemplo, todas as instâncias pertencem à mesma classe), um nó terminal é criado e atribuído a ele o valor mais comum da classe dentro desse grupo.\n",
        "\n",
        "def to_terminal(group):                               # Define uma função chamada to_terminal (grupo de instâncias para o qual o nó terminal será criado).\n",
        "    outcomes = [row[-1] for row in group]             # Obtém todas as classes das instâncias no grupo. O índice '-1' é usado para acessar a última coluna, que contém as classes.\n",
        "    return max(set(outcomes), key=outcomes.count)     # Retorna a classe mais comum no grupo, que será atribuída ao nó terminal da árvore de decisão."
      ],
      "metadata": {
        "id": "bOrKFYnKyw7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando divisões nos nós de uma árvore de decisão: --> responsável por dividir um nó da árvore em dois nós filhos, criando a estrutura de decisão recursivamente.\n",
        "\n",
        "def split(node, max_depth, min_size, n_features, depth): # Define uma função chamada split (nó atual que está sendo dividido, profundidade máxima permitida para a árvore, número mínimo de instâncias permitido em um nó antes de não ser mais dividido, número de características (atributos) a serem consideradas ao fazer uma divisão, profundidade atual na construção da árvore).\n",
        "\n",
        "    left, right = node['groups']                                                # Obtém os grupos resultantes da divisão do nó atual,\n",
        "    del(node['groups'])                                                         # e remove a chave 'groups' do dicionário node.\n",
        "\n",
        "    # check for a no split (Verifica se não há dados à esquerda ou à direita para dividir. Nesse caso, o nó atual é transformado em um nó terminal com o valor da classe mais comum entre os dados combinados à esquerda e à direita).\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "\n",
        "    # check for max depth (Verifica se a profundidade máxima da árvore foi atingida. Se sim, os nós à esquerda e à direita são transformados em nós terminais).\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "\n",
        "    # process left child (1- Verifica se o número de instâncias no nó à esquerda é menor ou igual ao tamanho mínimo especificado).\n",
        "    if len(left) <= min_size:                                             # Se sim, o nó à esquerda é transformado em um nó terminal.\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:                                                                 # Caso contrário, a função get_split é chamada para encontrar a melhor divisão para o nó à esquerda, e o processo é repetido recursivamente.\n",
        "        node['left'] = get_split(left, n_features)\n",
        "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
        "\n",
        "    # process right child (2- Verifica se o número de instâncias no nó à direita é menor ou igual ao tamanho mínimo especificado).\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right, n_features)\n",
        "        split(node['right'], max_depth, min_size, n_features, depth+1)"
      ],
      "metadata": {
        "id": "mHbvr1bxyx_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construindo uma árvore de decisão usando o algoritmo CART (Classification and Regression Trees):\n",
        "\n",
        "def build_tree(train, max_depth, min_size, n_features):      # Define uma função chamada build_tree (conjunto de treinamento usado para construir a árvore de decisão, profundidade máxima permitida para a árvore, número mínimo de instâncias permitido em um nó antes de não ser mais dividido, número de características (atributos) a serem consideradas ao fazer uma divisão).\n",
        "    root = get_split(dataset, n_features)                    # Usa a função get_split para encontrar a melhor divisão para o conjunto de treinamento, resultando no nó raiz da árvore de decisão.\n",
        "    split(root, max_depth, min_size, n_features, 1)          # Usa a função split para dividir recursivamente o nó raiz e seus filhos até atingir os critérios de parada especificados pelos parâmetros max_depth e min_size.\n",
        "    return root                                              # Retorna o nó raiz da árvore de decisão construída."
      ],
      "metadata": {
        "id": "6uxdbFUGy1Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo previsões usando uma árvore de decisão construída:\n",
        "\n",
        "def predict(node, row):                                                                 # Define uma função chamada predict (nó atual da árvore de decisão, instância de dados (vetor) para a qual uma previsão será feita).\n",
        "\n",
        "    if row[node['index']] < node['value']:                                              # Verifica se o valor do atributo na instância row é menor que o valor de divisão armazenado no nó. Se a condição for verdadeira, a travessia continua para o nó filho à esquerda.\n",
        "        if isinstance(node['left'], dict): return predict(node['left'], row)            # 1- Se o nó filho for um dicionário, isso significa que ainda não é um nó terminal, então a função predict é chamada recursivamente para continuar a travessia pela árvore.\n",
        "        else: return node['left']                                                       # 2- Se o nó filho for um valor terminal (ou seja, não é um dicionário), isso significa que é um nó terminal, e o valor do nó é retornado como a previsão.\n",
        "    else:                                                                               # Caso contrário, a travessia continua para o nó filho à direita.\n",
        "        if isinstance(node['right'], dict): return predict(node['right'], row)          # 1- ~~\n",
        "        else: return node['right']                                                      # 2- ~~"
      ],
      "metadata": {
        "id": "O7uTKJKTy7Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma amostra aleatória (subamostra) do conjunto de dados:\n",
        "\n",
        "def subsample(dataset, ratio):                   # Define uma função chamada subsample (conjunto de dados original do qual a subamostra será criada, proporção de instâncias que serão incluídas na subamostra, representada como um valor entre 0 e 1).\n",
        "    sample = list()                              # Cria uma lista vazia chamada sample, usada para armazenar as instâncias selecionadas aleatoriamente do conjunto de dados original.\n",
        "    n_sample = round(len(dataset) * ratio)       # Calcula o tamanho da subamostra multiplicando o número total de instâncias no conjunto de dados pelo valor da proporção ratio. O resultado é arredondado para o inteiro mais próximo.\n",
        "\n",
        "    while len(sample) < n_sample:                # Inicia um loop enquanto o tamanho da subamostra for menor que o tamanho desejado.\n",
        "        index = randrange(len(dataset))          # Seleciona aleatoriamente um índice do conjunto de dados original usando a função randrange.\n",
        "        sample.append(dataset[index])            # Adiciona a instância correspondente ao índice selecionado à subamostra.\n",
        "    return sample                                # Retorna a subamostra criada."
      ],
      "metadata": {
        "id": "As-Oahbly9j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo previsões utilizando uma lista de árvores de decisão que foram treinadas em diferentes subamostras do conjunto de dados original (método conhecido como \"bagging\" (Bootstrap Aggregating)):\n",
        "\n",
        "def bagging_predict(trees, row):                            # Define uma função chamada bagging_predict (lista de árvores de decisão previamente treinadas, instância de dados (vetor) para a qual uma previsão será feita).\n",
        "\n",
        "    predictions = [predict(tree, row) for tree in trees]    # Usa uma compreensão de lista para iterar sobre cada árvore na lista trees e faz uma previsão para a instância row usando a função predict.\n",
        "                                                            # Armazena todas as previsões em uma lista chamada predictions.\n",
        "\n",
        "    return max(set(predictions), key=predictions.count)     # Usa a função set para obter os valores únicos das previsões.\n",
        "                                                            # Usa a função max com o argumento key=predictions.count para encontrar a previsão mais comum (a moda) na lista de previsões.\n",
        "                                                            # Retorna a previsão mais comum como a previsão final para a instância row."
      ],
      "metadata": {
        "id": "lEY6GJBdy_ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Algoritmo Random Forest: --> O algoritmo cria várias árvores de decisão em diferentes subamostras do conjunto de dados original e combina suas previsões para obter uma previsão final robusta e precisa.\n",
        "\n",
        "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features)\n",
        "    trees = list()                                                                       # Cria uma lista vazia para armazenar as árvores de decisão que serão geradas.\n",
        "    for i in range(n_trees):                                                             # Para cada árvore na floresta (total de n_trees árvores):\n",
        "        sample = subsample(train, sample_size)                                           # Cria uma subamostra do conjunto de treinamento usando a função subsample com a proporção sample_size.\n",
        "        tree = build_tree(sample, max_depth, min_size, n_features)                       # Constrói uma árvore de decisão usando a função build_tree com a subamostra criada e os parâmetros max_depth, min_size e n_features.\n",
        "        trees.append(tree)                                                               # Adiciona a árvore construída à lista trees.\n",
        "    predictions = [bagging_predict(trees, row) for row in test]                          # Faz previsões para cada instância no conjunto de teste usando a função bagging_predict, que combina as previsões de todas as árvores na floresta, e armazena as previsões em uma lista predictions.\n",
        "    return(predictions)                                                                  # Retorna a lista de previsões para o conjunto de teste."
      ],
      "metadata": {
        "id": "El5fiTgRzCSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação Detalhada\n",
        "- Subamostragem: Cada árvore é treinada em uma subamostra do conjunto de dados de treinamento, criada com reposição (ou seja, a mesma instância pode aparecer várias vezes na mesma subamostra).\n",
        "\n",
        "- Seleção Aleatória de Características: Ao procurar a melhor divisão em cada nó de uma árvore, apenas um subconjunto aleatório de características é considerado (n_features). Isso promove a diversidade entre as árvores e ajuda a reduzir a correlação entre elas.\n",
        "\n",
        "- Combinação das Previsões: As previsões de todas as árvores são combinadas usando votação majoritária (para classificação) ou média (para regressão) para produzir a previsão final.\n",
        "\n",
        "Essa abordagem melhora a precisão e a robustez do modelo, tornando-o menos suscetível a overfitting em comparação com uma única árvore de decisão."
      ],
      "metadata": {
        "id": "1wJBH1mMnyp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando Algoritmo Random Forest:\n",
        "\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "dataset = load_csv(\"/content/car_accidentOFICIAL.csv\")\n",
        "# convert string attributes to integers\n",
        "for i in range(0, len(dataset[0])-1):\n",
        "    str_column_to_flt(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 10\n",
        "min_size = 1\n",
        "sample_size = 1.0\n",
        "n_features = int(sqrt(len(dataset[0])-1))\n",
        "for n_trees in [1, 5, 10]:\n",
        "    scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
        "    print('Trees: %d' % n_trees)\n",
        "    print('Scores: %s' % scores)\n",
        "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
        "    print('')"
      ],
      "metadata": {
        "id": "tt5n0x2GzEvw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}