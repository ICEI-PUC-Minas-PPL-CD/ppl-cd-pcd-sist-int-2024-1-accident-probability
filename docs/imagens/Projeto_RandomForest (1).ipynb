{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9RK7eTxIIxX"
      },
      "source": [
        "import pandas as pd                                                         # importando a biblioteca pandas (utilizada para manipulação e análise de dados)\n",
        "import numpy as np                                                          # importando a biblioteca numpy (usada para operações matemáticas e manipulação de arrays)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmo Random Forest\n",
        "\n",
        "## 1- Construção de Árvores de Decisão:\n",
        "\n",
        "A Random Forest constrói múltiplas árvores de decisão durante o tempo de treinamento. Uma Árvore de Decisão é um modelo preditivo que divide repetidamente os dados em subconjuntos baseados em características (features) específicas, visando reduzir a impureza dos dados em cada divisão."
      ],
      "metadata": {
        "id": "atNqQPAMkP1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Agregação por Bootstrap (Bagging):\n",
        "\n",
        "A técnica de agregação por bootstrap, ou simplesmente \"bagging\", é utilizada para treinar cada árvore. Isso envolve:\n",
        "\n",
        "- Amostragem com reposição: Para cada árvore, uma amostra aleatória do conjunto de treinamento original é extraída com reposição. Isso significa que alguns dados podem ser selecionados várias vezes, enquanto outros podem não ser selecionados.\n",
        "- Treinamento das Árvores: Cada árvore é treinada de forma independente usando essas amostras aleatórias."
      ],
      "metadata": {
        "id": "5aHyzxbxwxW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- Diversidade entre Árvores:\n",
        "\n",
        "Para aumentar a diversidade entre as árvores e evitar que todas sigam os mesmos padrões de divisão, a Random Forest introduz aleatoriedade adicional:\n",
        "\n",
        "- Subconjuntos de Características: Ao invés de considerar todas as características disponíveis para cada divisão, apenas um subconjunto aleatório de características é considerado. Esse subconjunto é selecionado aleatoriamente para cada divisão em cada árvore."
      ],
      "metadata": {
        "id": "4WFwfOxdxHyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4- Combinação de Previsões:\n",
        "\n",
        "Após o treinamento, as previsões de todas as árvores são combinadas para gerar a previsão final:\n",
        "- Classificação: A classe final é determinada pela moda das previsões de todas as árvores (a classe mais votada).\n",
        "- Regressão: A previsão final é a média das previsões de todas as árvores."
      ],
      "metadata": {
        "id": "Y3vLO-5Bxeqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed, randrange  # seed: define a semente para o gerador de números aleatórios, garantindo que ele produza a mesma sequência de números toda vez que o código for executado;\n",
        "                                    # randrange: gera um número inteiro aleatório dentro de um intervalo especificado;\n",
        "from csv import reader              # reader: função usada para ler dados de um arquivo CSV;\n",
        "from math import sqrt               # sqrt: função usada para calcular a raiz quadrada de um número."
      ],
      "metadata": {
        "id": "_zlB1MlKxr4F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando um arquivo CSV:\n",
        "\n",
        "def load_csv(filename):                          # Define uma função chamada load_csv para carregar o arquivo CSV a ser lido.\n",
        "    dataset = list()                             # Inicializa uma lista vazia chamada dataset, que será usada para armazenar as linhas do CSV.\n",
        "    with open(filename, 'r') as file:            # Abre o arquivo CSV para leitura ('r'). *O with statement garante que o arquivo será fechado automaticamente após a leitura.\n",
        "        csv_reader = reader(file)                # Cria um objeto 'csv_reader' usando a função 'reader' da biblioteca 'csv'. Este objeto permitirá iterar (percorrer ou passar por cada item de uma sequência de dados) sobre as linhas do arquivo CSV.\n",
        "        for row in csv_reader:                   # Inicia um loop 'for' que itera sobre cada linha (row) no csv_reader.\n",
        "            if not row: continue                 # Verifica se a linha (row) está vazia. Se estiver vazia, usa 'continue' para pular para a próxima iteração do loop, evitando adicionar linhas vazias ao dataset.\n",
        "            dataset.append(row)                  # Adiciona a linha (row) não vazia à lista dataset.\n",
        "    return dataset                               # Após ler todas as linhas do CSV e adicioná-las à lista dataset, a função retorna o dataset."
      ],
      "metadata": {
        "id": "IjuQTXhOyQ-K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/car_accidentOFICIAL.csv\")"
      ],
      "metadata": {
        "id": "8pG-tDMN27Sq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os valores de uma coluna de strings para floats:\n",
        "\n",
        "def str_column_to_flt(dataset, column):                          # Define a função 'chamada str_column_to_flt', que converte os valores de uma coluna específica em um dataset de strings para floats.\n",
        "    for row in dataset:                                          # Inicia um loop 'for' que itera sobre cada linha (row) no dataset.\n",
        "        row[column] = float( row[column].strip() )               # Para cada linha, acessa o valor na coluna especificada, remove quaisquer espaços em branco usando strip(), converte o valor para float, e atualiza o valor na linha."
      ],
      "metadata": {
        "id": "aQ07JwnmyTvg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os valores de uma coluna de strings para inteiros:\n",
        "\n",
        "def str_column_to_int(dataset, column):\n",
        "    class_values = [ row[column] for row in dataset ]\n",
        "    unique = set( class_values )\n",
        "    lookup = dict()\n",
        "    for i, value in enumerate( unique ): lookup[value] = i\n",
        "    for row in dataset: row[column] = lookup[row[column]]\n",
        "    return lookup"
      ],
      "metadata": {
        "id": "mmS4UEWRyXV0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo um dataset em k folds para validação cruzada (usada para avaliar a capacidade de generalização de um modelo):\n",
        "\n",
        "from random import randrange\n",
        "\n",
        "def cross_validation_split(dataset, n_folds):            # Define uma função chamada cross_validation_split (dataset que será dividido em folds, número de folds desejado para a divisão do dataset).\n",
        "                                                           # Inicialização das variáveis:\n",
        "    dataset_split = list()                               # dataset_split: uma lista que armazenará os folds criados.\n",
        "    dataset_copy = list(dataset)                         # dataset_copy: uma cópia do dataset original, feito para garantir que não haja alterações no dataset original durante a divisão.\n",
        "    fold_size = len(dataset) / n_folds                   # fold_size: tamanho de cada fold (é calculado dividindo o número total de exemplos pelo número de folds desejado).\n",
        "\n",
        "    for i in range(n_folds):                             # Inicia um loop que será executado n vezes, criando um fold em cada iteração.\n",
        "        fold = list()                                    # Inicializa uma lista vazia para armazenar os exemplos do fold atual.\n",
        "        while len(fold) < fold_size:                     # Enquanto o tamanho do fold atual for menor que o tamanho esperado (fold_size), um loop é executado.\n",
        "            index = randrange(len(dataset_copy))         # Um índice aleatório é escolhido dentro do intervalo válido para o dataset atual (usando randrange(len(dataset_copy))).\n",
        "            fold.append(dataset_copy.pop(index))         # O exemplo correspondente ao índice escolhido é removido de dataset_copy (usando pop(index)) e adicionado ao fold atual.\n",
        "\n",
        "        dataset_split.append(fold)                       # Após preencher o fold atual com exemplos suficientes, ele é adicionado à lista de folds (dataset_split).\n",
        "    return dataset_split                                 # Após criar todos os folds, a função retorna a lista de folds.\n"
      ],
      "metadata": {
        "id": "mVfT5Qf9ya3p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando a precisão do modelo de classificação:\n",
        "\n",
        "def accuracy_metric(actual, predicted):                 # Define uma função chamada accuracy_metric (lista de valores reais, lista de valores previstos pelo modelo).\n",
        "    correct = 0                                         # Inicializa um contador para o número de previsões corretas.\n",
        "    for i in range(len(actual)):                        # Inicia um loop que itera sobre cada par de valores real e previsto.\n",
        "        if actual[i] == predicted[i]: correct += 1      # Verifica se a previsão (predicted[i]) coincide com o valor real (actual[i]), e, se sim, incrementa o contador 'correct'.\n",
        "    return 100. * correct / float( len(actual) )        # Retorna a precisão calculada como a porcentagem de previsões corretas em relação ao total de previsões.\n",
        "                                                        # (multiplica por 100 para obter o valor em percentual)"
      ],
      "metadata": {
        "id": "ypX33Cmpyfvp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliando o algoritmo de aprendizado de máquina usando divisão de validação cruzada:\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):      # Define uma função chamada evaluate_algorithm (dataset, algoritmo de aprendizado de máquina a ser avaliado, número de folds para a validação cruzada, argumentos adicionais que podem ser passados para o algoritmo).\n",
        "    folds = cross_validation_split(dataset, n_folds)             # Usa a função cross_validation_split para dividir o dataset em k (número de subconjuntos) folds.\n",
        "    scores = list()                                              # Inicializa uma lista vazia para armazenar as pontuações de precisão de cada fold.\n",
        "    for fold in folds:                                           # Inicia um loop que itera sobre cada fold criado pela função cross_validation_split.\n",
        "\n",
        "        train_set = list(folds)                                  # Cria o conjunto de treinamento combinando todos os folds, exceto o fold atual.\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "\n",
        "        test_set = list()                                        # Cria o conjunto de teste para o fold atual, removendo o valor de destino para evitar vazamento de informações durante o teste.\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "\n",
        "        predicted = algorithm(train_set, test_set, *args)        # Executa o algoritmo de aprendizado de máquina no conjunto de treinamento e teste, passando quaisquer argumentos adicionais especificados.\n",
        "\n",
        "        actual = [row[-1] for row in fold]                       # Obtém os valores reais do conjunto de teste e calcula a precisão das previsões feitas pelo algoritmo.\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "\n",
        "        scores.append(accuracy)                                  # Adiciona a precisão à lista de pontuações.\n",
        "    return scores                                                # Retorna a lista de pontuações de precisão para todos os folds."
      ],
      "metadata": {
        "id": "vuTkYAY2yjvJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo um dataset em dois grupos com base no valor de um atributo específico:\n",
        "\n",
        "def test_split(index, value, dataset):                          # Define uma função chamada test_split (índice do atributo usado para a divisão, valor do atributo que determina a divisão, dataset a ser dividido).\n",
        "    left, right = list(), list()                                # Inicializa duas listas vazias para os grupos resultantes da divisão: um grupo à esquerda e outro à direita.\n",
        "    for row in dataset:                                         # Inicia um loop que itera sobre cada linha do dataset.\n",
        "        if row[index] < value: left.append(row)                 # Verifica se o valor do atributo na linha é menor que o valor especificado (value), se for, a linha é adicionada ao grupo à esquerda (left).\n",
        "        else: right.append(row)                                 # Caso contrário, a linha é adicionada ao grupo à direita (right).\n",
        "    return left, right                                          # Retorna os grupos resultantes à esquerda e à direita"
      ],
      "metadata": {
        "id": "c1UFD3_UymhO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando o índice Gini (métrica de impureza usada em algoritmos de árvore de decisão para avaliar a qualidade de uma divisão em um dataset) para uma divisão de dataset:\n",
        "\n",
        "def gini_index(groups, class_values):                                                    # Define uma função chamada gini_index (lista de grupos após a divisão do dataset, lista dos valores de classe possíveis).\n",
        "    gini = 0.0                                                                           # Inicializa o índice Gini como 0.\n",
        "    for class_value in class_values:                                                     # Inicia um loop que itera sobre cada valor de classe possível.\n",
        "        for group in groups:                                                             # Inicia um loop que itera sobre cada grupo na divisão do dataset.\n",
        "            size = len(group)                                                            # Calcula o tamanho do grupo atual.\n",
        "            if size == 0: continue                                                       # Verifica se o grupo está vazio. Se estiver, passa para o próximo grupo.\n",
        "            proportion = [row[-1] for row in group].count(class_value) / float(size)     # Calcula a proporção de instâncias no grupo atual que têm o valor de classe atual.\n",
        "            gini += (proportion * (1.0 - proportion))                                    # Adiciona a contribuição do grupo para o índice Gini. Quanto maior a impureza do grupo, maior será a contribuição para o índice Gini.\n",
        "    return gini                                                                          # Retorna o índice Gini final."
      ],
      "metadata": {
        "id": "CVDx7UMDyp7U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionar o melhor ponto de divisão para o dataset: (objetivo é encontrar a melhor maneira de dividir o dataset com base em determinadas características (atributos) para maximizar a pureza dos grupos resultantes).\n",
        "\n",
        "def get_split(dataset, n_features):                                                      # Define uma função chamada get_split (dataset para o qual o ponto de divisão será selecionado, número de atributos a serem considerados durante a seleção do ponto de divisão).\n",
        "    class_values = list(set(row[-1] for row in dataset))                                 # Obtém os valores de classe únicos no dataset.\n",
        "                                                                                            # Inicialização das variáveis para armazenar o melhor ponto de divisão encontrado até agora:\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None                            # b_index: Índice do atributo usado para a divisão. / b_value: Valor do atributo usado para a divisão. / b_score: Índice Gini do melhor ponto de divisão encontrado até agora. / b_groups: Grupos resultantes da melhor divisão encontrada até agora.\n",
        "\n",
        "    features = list()                                                                    # Seleciona aleatoriamente 'n' índices de atributos para consideração durante a seleção do ponto de divisão.\n",
        "    while len(features) < n_features:\n",
        "        index = randrange(len(dataset[0])-1)\n",
        "        if index not in features: features.append(index)\n",
        "\n",
        "    for index in features:                                                               # Inicia um loop que itera sobre cada índice de atributo selecionado.\n",
        "        for row in dataset:                                                              # Inicia um loop que itera sobre cada linha no dataset.\n",
        "            groups = test_split(index, row[index], dataset)                              # Divide o dataset com base no atributo atual e no valor do atributo na linha atual.\n",
        "            gini = gini_index(groups, class_values)                                      # Calcula o índice Gini para a divisão.\n",
        "            if gini < b_score:                                                           # Se o índice Gini calculado for menor que o melhor índice Gini encontrado até agora,\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups    # atualiza as variáveis para o melhor ponto de divisão encontrado até agora.\n",
        "\n",
        "    return {'index': b_index, 'value': b_value, 'groups': b_groups}                      # Retorna um dicionário contendo o índice do atributo usado para a divisão, o valor do atributo usado para a divisão e os grupos resultantes da divisão."
      ],
      "metadata": {
        "id": "WsedQRB9ys7Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando um nó terminal em uma árvore de decisão:  --> quando a construção da árvore de decisão atinge um ponto onde não há mais divisões a serem feitas (por exemplo, todas as instâncias pertencem à mesma classe), um nó terminal é criado e atribuído a ele o valor mais comum da classe dentro desse grupo.\n",
        "\n",
        "def to_terminal(group):                               # Define uma função chamada to_terminal (grupo de instâncias para o qual o nó terminal será criado).\n",
        "    outcomes = [row[-1] for row in group]             # Obtém todas as classes das instâncias no grupo. O índice '-1' é usado para acessar a última coluna, que contém as classes.\n",
        "    return max(set(outcomes), key=outcomes.count)     # Retorna a classe mais comum no grupo, que será atribuída ao nó terminal da árvore de decisão."
      ],
      "metadata": {
        "id": "bOrKFYnKyw7d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando divisões nos nós de uma árvore de decisão: --> responsável por dividir um nó da árvore em dois nós filhos, criando a estrutura de decisão recursivamente.\n",
        "\n",
        "def split(node, max_depth, min_size, n_features, depth): # Define uma função chamada split (nó atual que está sendo dividido, profundidade máxima permitida para a árvore, número mínimo de instâncias permitido em um nó antes de não ser mais dividido, número de características (atributos) a serem consideradas ao fazer uma divisão, profundidade atual na construção da árvore).\n",
        "\n",
        "    left, right = node['groups']                                                # Obtém os grupos resultantes da divisão do nó atual,\n",
        "    del(node['groups'])                                                         # e remove a chave 'groups' do dicionário node.\n",
        "\n",
        "    # check for a no split (Verifica se não há dados à esquerda ou à direita para dividir. Nesse caso, o nó atual é transformado em um nó terminal com o valor da classe mais comum entre os dados combinados à esquerda e à direita).\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "\n",
        "    # check for max depth (Verifica se a profundidade máxima da árvore foi atingida. Se sim, os nós à esquerda e à direita são transformados em nós terminais).\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "\n",
        "    # process left child (1- Verifica se o número de instâncias no nó à esquerda é menor ou igual ao tamanho mínimo especificado).\n",
        "    if len(left) <= min_size:                                             # Se sim, o nó à esquerda é transformado em um nó terminal.\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:                                                                 # Caso contrário, a função get_split é chamada para encontrar a melhor divisão para o nó à esquerda, e o processo é repetido recursivamente.\n",
        "        node['left'] = get_split(left, n_features)\n",
        "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
        "\n",
        "    # process right child (2- Verifica se o número de instâncias no nó à direita é menor ou igual ao tamanho mínimo especificado).\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right, n_features)\n",
        "        split(node['right'], max_depth, min_size, n_features, depth+1)"
      ],
      "metadata": {
        "id": "mHbvr1bxyx_M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construindo uma árvore de decisão usando o algoritmo CART (Classification and Regression Trees):\n",
        "\n",
        "def build_tree(train, max_depth, min_size, n_features):      # Define uma função chamada build_tree (conjunto de treinamento usado para construir a árvore de decisão, profundidade máxima permitida para a árvore, número mínimo de instâncias permitido em um nó antes de não ser mais dividido, número de características (atributos) a serem consideradas ao fazer uma divisão).\n",
        "    root = get_split(dataset, n_features)                    # Usa a função get_split para encontrar a melhor divisão para o conjunto de treinamento, resultando no nó raiz da árvore de decisão.\n",
        "    split(root, max_depth, min_size, n_features, 1)          # Usa a função split para dividir recursivamente o nó raiz e seus filhos até atingir os critérios de parada especificados pelos parâmetros max_depth e min_size.\n",
        "    return root                                              # Retorna o nó raiz da árvore de decisão construída."
      ],
      "metadata": {
        "id": "6uxdbFUGy1Uc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo previsões usando uma árvore de decisão construída:\n",
        "\n",
        "def predict(node, row):                                                                 # Define uma função chamada predict (nó atual da árvore de decisão, instância de dados (vetor) para a qual uma previsão será feita).\n",
        "\n",
        "    if row[node['index']] < node['value']:                                              # Verifica se o valor do atributo na instância row é menor que o valor de divisão armazenado no nó. Se a condição for verdadeira, a travessia continua para o nó filho à esquerda.\n",
        "        if isinstance(node['left'], dict): return predict(node['left'], row)            # 1- Se o nó filho for um dicionário, isso significa que ainda não é um nó terminal, então a função predict é chamada recursivamente para continuar a travessia pela árvore.\n",
        "        else: return node['left']                                                       # 2- Se o nó filho for um valor terminal (ou seja, não é um dicionário), isso significa que é um nó terminal, e o valor do nó é retornado como a previsão.\n",
        "    else:                                                                               # Caso contrário, a travessia continua para o nó filho à direita.\n",
        "        if isinstance(node['right'], dict): return predict(node['right'], row)          # 1- ~~\n",
        "        else: return node['right']                                                      # 2- ~~"
      ],
      "metadata": {
        "id": "O7uTKJKTy7Pr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma amostra aleatória (subamostra) do conjunto de dados:\n",
        "\n",
        "def subsample(dataset, ratio):                   # Define uma função chamada subsample (conjunto de dados original do qual a subamostra será criada, proporção de instâncias que serão incluídas na subamostra, representada como um valor entre 0 e 1).\n",
        "    sample = list()                              # Cria uma lista vazia chamada sample, usada para armazenar as instâncias selecionadas aleatoriamente do conjunto de dados original.\n",
        "    n_sample = round(len(dataset) * ratio)       # Calcula o tamanho da subamostra multiplicando o número total de instâncias no conjunto de dados pelo valor da proporção ratio. O resultado é arredondado para o inteiro mais próximo.\n",
        "\n",
        "    while len(sample) < n_sample:                # Inicia um loop enquanto o tamanho da subamostra for menor que o tamanho desejado.\n",
        "        index = randrange(len(dataset))          # Seleciona aleatoriamente um índice do conjunto de dados original usando a função randrange.\n",
        "        sample.append(dataset[index])            # Adiciona a instância correspondente ao índice selecionado à subamostra.\n",
        "    return sample                                # Retorna a subamostra criada."
      ],
      "metadata": {
        "id": "As-Oahbly9j6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo previsões utilizando uma lista de árvores de decisão que foram treinadas em diferentes subamostras do conjunto de dados original (método conhecido como \"bagging\" (Bootstrap Aggregating)):\n",
        "\n",
        "def bagging_predict(trees, row):                            # Define uma função chamada bagging_predict (lista de árvores de decisão previamente treinadas, instância de dados (vetor) para a qual uma previsão será feita).\n",
        "\n",
        "    predictions = [predict(tree, row) for tree in trees]    # Usa uma compreensão de lista para iterar sobre cada árvore na lista trees e faz uma previsão para a instância row usando a função predict.\n",
        "                                                            # Armazena todas as previsões em uma lista chamada predictions.\n",
        "\n",
        "    return max(set(predictions), key=predictions.count)     # Usa a função set para obter os valores únicos das previsões.\n",
        "                                                            # Usa a função max com o argumento key=predictions.count para encontrar a previsão mais comum (a moda) na lista de previsões.\n",
        "                                                            # Retorna a previsão mais comum como a previsão final para a instância row."
      ],
      "metadata": {
        "id": "lEY6GJBdy_ww"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Algoritmo Random Forest: --> O algoritmo cria várias árvores de decisão em diferentes subamostras do conjunto de dados original e combina suas previsões para obter uma previsão final robusta e precisa.\n",
        "\n",
        "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
        "    trees = list()                                                                       # Cria uma lista vazia para armazenar as árvores de decisão que serão geradas.\n",
        "    for i in range(n_trees):                                                             # Para cada árvore na floresta (total de n_trees árvores):\n",
        "        sample = subsample(train, sample_size)                                           # Cria uma subamostra do conjunto de treinamento usando a função subsample com a proporção sample_size.\n",
        "        tree = build_tree(sample, max_depth, min_size, n_features)                       # Constrói uma árvore de decisão usando a função build_tree com a subamostra criada e os parâmetros max_depth, min_size e n_features.\n",
        "        trees.append(tree)                                                               # Adiciona a árvore construída à lista trees.\n",
        "    predictions = [bagging_predict(trees, row) for row in test]                          # Faz previsões para cada instância no conjunto de teste usando a função bagging_predict, que combina as previsões de todas as árvores na floresta, e armazena as previsões em uma lista predictions.\n",
        "    return(predictions)                                                                  # Retorna a lista de previsões para o conjunto de teste."
      ],
      "metadata": {
        "id": "El5fiTgRzCSs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação Detalhada\n",
        "- Subamostragem: Cada árvore é treinada em uma subamostra do conjunto de dados de treinamento, criada com reposição (ou seja, a mesma instância pode aparecer várias vezes na mesma subamostra).\n",
        "\n",
        "- Seleção Aleatória de Características: Ao procurar a melhor divisão em cada nó de uma árvore, apenas um subconjunto aleatório de características é considerado (n_features). Isso promove a diversidade entre as árvores e ajuda a reduzir a correlação entre elas.\n",
        "\n",
        "- Combinação das Previsões: As previsões de todas as árvores são combinadas usando votação majoritária (para classificação) ou média (para regressão) para produzir a previsão final.\n",
        "\n",
        "Essa abordagem melhora a precisão e a robustez do modelo, tornando-o menos suscetível a overfitting em comparação com uma única árvore de decisão."
      ],
      "metadata": {
        "id": "1wJBH1mMnyp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminando a coluna Accident_ID\n",
        "dataset = dataset.drop['Accident_ID']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "EsgUtq_5G8AI",
        "outputId": "21899a89-280e-40ab-b82f-b3680d09b21f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'method' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-401d1bf91638>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Eliminando a coluna Accident_ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Accident_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.head())  # Exibindo as primeiras linhas do DataFrame para checar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syUG_V1XHIDi",
        "outputId": "fa9f4578-3c2e-4ea6-fd2f-e4ce285af225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Count Accident_Date Day_of_Week          Junction_Detail Accident_Injuries  \\\n",
            "0      0    2021-01-01    Thursday  T or staggered junction           Serious   \n",
            "1      1    2021-01-05      Monday               Crossroads           Serious   \n",
            "2      2    2021-01-04      Sunday  T or staggered junction            Slight   \n",
            "3      3    2021-01-05      Monday  T or staggered junction           Serious   \n",
            "4      4    2021-01-06     Tuesday               Crossroads           Serious   \n",
            "\n",
            "        Light_Conditions  Number_of_Casualties  Number_of_Vehicles  \\\n",
            "0               Daylight                     1                   2   \n",
            "1               Daylight                    11                   2   \n",
            "2               Daylight                     1                   2   \n",
            "3               Daylight                     1                   2   \n",
            "4  Darkness - lights lit                     1                   2   \n",
            "\n",
            "  Road_Surface_Conditions           Road_Type  Speed_limit Hour_of_Accident  \\\n",
            "0                     Dry      One way street           30            12-15   \n",
            "1             Wet or damp  Single carriageway           30             9-12   \n",
            "2                     Dry  Single carriageway           30            12-15   \n",
            "3            Frost or ice  Single carriageway           30              6-9   \n",
            "4                     Dry  Single carriageway           30            15-18   \n",
            "\n",
            "  Urban_or_Rural_Area  Weather_Conditions Vehicle_Type  \n",
            "0               Urban  Fine no high winds          Car  \n",
            "1               Urban  Fine no high winds         Taxi  \n",
            "2               Urban  Fine no high winds         Taxi  \n",
            "3               Urban               Other   Motorcycle  \n",
            "4               Urban  Fine no high winds          Car  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando Algoritmo Random Forest:\n",
        "\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "dataset = load_csv(\"/content/car_accidentOFICIAL.csv\")\n",
        "# convert string attributes to integers\n",
        "for i in range(0, len(dataset[0])-1):\n",
        "    str_column_to_flt(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 10\n",
        "min_size = 1\n",
        "sample_size = 1.0\n",
        "n_features = int(sqrt(len(dataset[0])-1))\n",
        "for n_trees in [1, 5, 10]:\n",
        "    scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
        "    print('Trees: %d' % n_trees)\n",
        "    print('Scores: %s' % scores)\n",
        "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
        "    print('')"
      ],
      "metadata": {
        "id": "tt5n0x2GzEvw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "a233c963-29b8-4725-abf9-584d5f4d77b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'Count'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-874401e34bd1>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# convert string attributes to integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstr_column_to_flt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# convert class column to integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstr_column_to_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2e3f99fe80c0>\u001b[0m in \u001b[0;36mstr_column_to_flt\u001b[0;34m(dataset, column)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstr_column_to_flt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                          \u001b[0;31m# Define a função 'chamada str_column_to_flt', que converte os valores de uma coluna específica em um dataset de strings para floats.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m                                          \u001b[0;31m# Inicia um loop 'for' que itera sobre cada linha (row) no dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m               \u001b[0;31m# Para cada linha, acessa o valor na coluna especificada, remove quaisquer espaços em branco usando strip(), converte o valor para float, e atualiza o valor na linha.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Count'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carregando os dados\n",
        "dataset = pd.read_csv(\"/content/car_accidentOFICIAL.csv\")\n",
        "\n",
        "# Separando as features (atributos) e o target (classe)\n",
        "X = dataset.drop(columns=['Accident_Injuries'])  # Features\n",
        "y = dataset['Accident_Injuries']  # Variável de destino (classe)\n",
        "\n",
        "# Dividindo os dados em conjuntos de treinamento e teste (por exemplo, 80% para treinamento e 20% para teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Definindo o modelo de Random Forest\n",
        "random_forest_model = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=1)\n",
        "\n",
        "# Treinando o modelo\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões no conjunto de teste\n",
        "y_pred = random_forest_model.predict(X_test)\n",
        "\n",
        "# Avaliando a precisão do modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Acurácia:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "OfWuNb_WBylF",
        "outputId": "a718af68-aa7c-4b6a-e687-1079cc332c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '201043R088080'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-fdd6ddffbf90>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Treinando o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mrandom_forest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Fazendo previsões no conjunto de teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1996\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         if (\n\u001b[1;32m   2000\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '201043R088080'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carregando os dados\n",
        "# Supondo que você já carregou seus dados em um DataFrame chamado 'dataset'\n",
        "\n",
        "# Separando as features (atributos) e o target (classe)\n",
        "X = dataset.drop(columns=['Accident_Injuries'])\n",
        "y = dataset['Accident_Injuries']\n",
        "\n",
        "# Dividindo os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Definindo o modelo Random Forest\n",
        "n_estimators = 10  # Número de árvores na floresta\n",
        "max_depth = 10  # Profundidade máxima das árvores\n",
        "random_forest_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=1)\n",
        "\n",
        "# Treinando o modelo\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões\n",
        "y_pred = random_forest_model.predict(X_test)\n",
        "\n",
        "# Avaliando a precisão do modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "zvhkb5t18dSf",
        "outputId": "4a70d804-1e38-49d1-8703-576d244d1dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '201043R088080'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-5f56e9a9a715>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Treinando o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrandom_forest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Fazendo previsões\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1996\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         if (\n\u001b[1;32m   2000\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '201043R088080'"
          ]
        }
      ]
    }
  ]
}